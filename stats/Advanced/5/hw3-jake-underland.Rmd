---
title: "Homework Assignment 02"
subtitle: "Numerical Statistics Fall, 2022"
author: "Jake Underland - 1A193008"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    latex_engine: pdflatex
    toc: false
    extra_dependencies: ["amsmath", "xcolor", "bm"]
    keep_tex: yes
  html_document:
    toc: false
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: false
    toc_depth: '3'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, echo = FALSE}
library(reticulate)
```


# 1.  
Suppose that five people, $a, b, c, d$ and $e$, entered their photos in an exhibition, and judges $X$ and $Y$ scored the photos within the range of $0$ to $50$ points and $0$ to $100$ points, respectively:  
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & a & b & c & d & e\\
\hline
X & 32 & 18 & 39 & 47 & 26\\
\hline
Y & 90 & 65 & 45 & 80 & 70 \\
\hline
\end{tabular}
\end{center}
In this case calculate the Spearman rank correlation coefficient $r_s$, and then express it as an irreducible fraction.  
  

**Solution**:  
The table converted to express only the ordinal evaluations of $X$ and $Y$ is as below: 
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & a & b & c & d & e\\
\hline
X & 3 & 1 & 4 & 5 & 2\\
\hline
Y & 5 & 2 & 1 & 4 & 3 \\
\hline
\end{tabular}
\end{center}

The formula for Spearman rank correlation coefficient is 
$$r_s = 1 - \frac{6}{n^3-n} \sum^n_{i=1}(R_i - R_i')^2$$
Thus, plugging the ranked data in we get 
$$
\begin{aligned}
r_s &= 1 - \frac{6}{5^3-5} \left\{(3-5)^2 + (1 - 2)^2 + (4-1)^2 + (5-4)^2 + (2-3)^2\right\} \\
&= \frac{24}{120} = \frac{1}{5}
\end{aligned}
$$
Code for calculating the Spearman rank correlation coefficient is below. 
```{python}
import numpy as np
X = [32, 18, 39, 47, 26]
Y = [90, 65, 45, 80, 70]

def rank_vector(vec): 
    return [(sorted(vec).index(x) + 1) for x in vec]
    
def compute_spearman(X, Y):
    '''
    inputs: two vectors
    outputs: spearman coefficient of two vectors
    '''
    assert len(X) == len(Y)
    n = len(X)
    data = np.array([rank_vector(X), rank_vector(Y)])
    rs = 1 - (6 / (n**3 - n)) * sum((data[0,] - data[1,])**2)
    return rs
```


# 2.  

Consider the following data:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
x & -1 & 0 & 1 & 3 & 4 & 5\\
\hline
y & 8 & 6 & 5 & 2 & 0 & -3\\
\hline
\end{tabular}
\end{center}

\begin{enumerate}
\item[(2-1)] Find the linear regression line, $y = \alpha + \beta x$, of $y$ on $x$ based on the data, and then express $\alpha$ and $\beta$ as irreducible fractions.
\end{enumerate}
\textbf{Solution.}  

In the following simple regression model $y = \beta_0 + \beta_1 x + e_i$, the best linear predictors for the two coefficients under squared loss are$(\hat{\beta_0}, \hat{\beta_1}) \in \arg \! \min_{\beta_0, \beta_1} \sum_i(y_i - \beta_0 - \beta_1 x_i)^2$. We can compute these as, 
$$ \begin{aligned}
\hat{\beta_0} &= \bar{y} - \hat{\beta_1}\bar{x}\\
\hat{\beta_1} &= \frac{\sum_{i=1}(y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}(x_i  - \bar{x})^2 }
\end{aligned}$$
Then, following the above formula, 
$$\begin{aligned}
\bar{x} &= 2\\
\bar{y} &= 3\\
\beta &= \frac{(-1 - 2)(8-3) + (0 - 2)(6 - 3) + (1 - 2)(5 - 3) + (3-2)(2-3) + (4 - 2)(0 - 3) + (5 - 2)(-3 - 3)}{(-1 - 2)^2 + (0 - 2)^2 + (1 - 2)^2 + (3-2)^2 + (4 - 2)^2 + (5 - 2)^2} \\
&= \frac{-12}{7} \\
\alpha &= 3 - \frac{-12}{7} \times 2 = \frac{45}{7} \dots \Box
\end{aligned}$$
Code can be found below: 
```{python, include=TRUE, echo=TRUE}
def estimate_beta1(x, y):
    x = np.array(x)
    y = np.array(y)
    numerator = np.sum((x - np.mean(x)) * (y - np.mean(y)))
    denominator = np.sum((x - np.mean(x)) ** 2)
    return reduce_frac(numerator, denominator)

def reduce_frac(num, denom):
    x = gcd(num, denom)
    return "{}/{}".format(num/x, denom/x)
    
def gcd(m, n):
    r = m % n
    return n if not r else gcd(n, r)
```
```{python}
x = [-1, 0, 1, 3, 4, 5]
y = [8, 6, 5, 2, 0, -3]
print(estimate_beta1(x, y))
```



```{r, include=FALSE, echo=FALSE}
x <- c(-1, 0, 1, 3, 4, 5)
y <- c(8, 6, 5, 2, 0, -3)
model <- lm(y ~ x)
summary(model)
-12 / 7
```

\begin{enumerate}
\item[(2-2)] Find the linear regression line, $x = \gamma + \delta y$ of $x$ on $y$ based on the data, and then express $\gamma$ and $\delta$ as irreducible fractions.
\end{enumerate}
\textbf{Solution.}  
Similarly, 
```{python}
print(estimate_beta1(y, x))
```
Thus, 
$$\begin{aligned}
\delta &= -\frac{4}{7} \\
\gamma &= 2 - \frac{-4}{7} \times 3 = \frac{26}{7}
\end{aligned}$$

```{r, include=FALSE, echo=FALSE}
model <- lm(x ~ y)
summary(model)
```



